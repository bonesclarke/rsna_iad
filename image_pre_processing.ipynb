{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import timm\n",
    "from typing import Optional, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598cb18",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77449f91",
   "metadata": {},
   "source": [
    "## DICOM Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a430609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_array(dcm_path: str) -> np.ndarray:\n",
    "    \"\"\"Load and process DICOM file to numpy array\"\"\"\n",
    "    dicom = pydicom.dcmread(dcm_path)\n",
    "    \n",
    "    # Apply VOI LUT (if available) for proper windowing\n",
    "    data = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    \n",
    "    # Handle slope and intercept\n",
    "    if hasattr(dicom, 'RescaleSlope') and hasattr(dicom, 'RescaleIntercept'):\n",
    "        data = data * dicom.RescaleSlope + dicom.RescaleIntercept\n",
    "    \n",
    "    return data.astype(np.float32)\n",
    "\n",
    "def apply_windowing(img: np.ndarray, \n",
    "                   window_center: float, \n",
    "                   window_width: float) -> np.ndarray:\n",
    "    \"\"\"Apply CT windowing for better visualization\"\"\"\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def get_brain_windows(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply multiple brain-specific CT windows\"\"\"\n",
    "    # Common brain CT windows\n",
    "    windows = {\n",
    "        'brain': (40, 80),      # Brain tissue\n",
    "        'subdural': (80, 200),  # Subdural window\n",
    "        'stroke': (40, 40),     # Stroke window\n",
    "        'aneurysm': (50, 150),  # Aneurysm visualization\n",
    "    }\n",
    "    \n",
    "    windowed_images = []\n",
    "    for name, (center, width) in windows.items():\n",
    "        windowed = apply_windowing(img.copy(), center, width)\n",
    "        # Normalize to 0-255\n",
    "        windowed = ((windowed - windowed.min()) / \n",
    "                   (windowed.max() - windowed.min() + 1e-8) * 255).astype(np.uint8)\n",
    "        windowed_images.append(windowed)\n",
    "    \n",
    "    # Stack as multi-channel image (can select 3 for RGB)\n",
    "    return np.stack(windowed_images[:3], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f51ed0",
   "metadata": {},
   "source": [
    "## Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a62c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hounsfield(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize Hounsfield units to 0-1 range\"\"\"\n",
    "    # Typical HU range for brain CT\n",
    "    MIN_HU = -100\n",
    "    MAX_HU = 200\n",
    "    \n",
    "    img = np.clip(img, MIN_HU, MAX_HU)\n",
    "    img = (img - MIN_HU) / (MAX_HU - MIN_HU)\n",
    "    return img\n",
    "\n",
    "def remove_skull_artifacts(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Simple skull stripping using morphological operations\"\"\"\n",
    "    # Convert to uint8 for OpenCV operations\n",
    "    img_uint8 = (img * 255).astype(np.uint8) if img.max() <= 1 else img.astype(np.uint8)\n",
    "    \n",
    "    # Threshold to get brain region\n",
    "    _, binary = cv2.threshold(img_uint8, 30, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Morphological operations to clean up\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    # Find largest contour (brain)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        mask = np.zeros_like(binary)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, 255, -1)\n",
    "        \n",
    "        # Apply mask\n",
    "        img_masked = cv2.bitwise_and(img_uint8, img_uint8, mask=mask)\n",
    "        return img_masked\n",
    "    return img_uint8\n",
    "\n",
    "def resize_with_padding(img: np.ndarray, \n",
    "                       target_size: Tuple[int, int],\n",
    "                       pad_value: int = 0) -> np.ndarray:\n",
    "    \"\"\"Resize image while maintaining aspect ratio with padding\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    target_h, target_w = target_size\n",
    "    \n",
    "    # Calculate scale to fit within target size\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    \n",
    "    # Resize image\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Create padded image\n",
    "    if len(img.shape) == 3:\n",
    "        padded = np.full((target_h, target_w, img.shape[2]), pad_value, dtype=img.dtype)\n",
    "    else:\n",
    "        padded = np.full((target_h, target_w), pad_value, dtype=img.dtype)\n",
    "    \n",
    "    # Center the resized image\n",
    "    y_offset = (target_h - new_h) // 2\n",
    "    x_offset = (target_w - new_w) // 2\n",
    "    padded[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a220a8a",
   "metadata": {},
   "source": [
    "## Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(img_size: int = 384):\n",
    "    \"\"\"Training augmentations optimized for medical images\"\"\"\n",
    "    return A.Compose([\n",
    "        # Spatial augmentations\n",
    "        A.RandomResizedCrop(img_size, img_size, scale=(0.8, 1.0), p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1, \n",
    "            scale_limit=0.15, \n",
    "            rotate_limit=15, \n",
    "            p=0.5,\n",
    "            border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        \n",
    "        # Intensity augmentations (careful with medical images)\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.1, \n",
    "                contrast_limit=0.1, \n",
    "                p=1.0\n",
    "            ),\n",
    "            A.RandomGamma(gamma_limit=(90, 110), p=1.0),\n",
    "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),\n",
    "        ], p=0.5),\n",
    "        \n",
    "        # Noise and blur (minimal for medical images)\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(5.0, 15.0), p=1.0),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        # Elastic deformation (useful for brain images)\n",
    "        A.ElasticTransform(\n",
    "            alpha=10, \n",
    "            sigma=5, \n",
    "            alpha_affine=0, \n",
    "            p=0.3\n",
    "        ),\n",
    "        \n",
    "        # Normalize based on model requirements\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(img_size: int = 384):\n",
    "    \"\"\"Validation/test augmentations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44079d",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AneurysmDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 img_dir: str,\n",
    "                 transforms=None,\n",
    "                 use_windowing: bool = True,\n",
    "                 img_size: int = 384):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.use_windowing = use_windowing\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load DICOM\n",
    "        img_path = f\"{self.img_dir}/{row['image_id']}.dcm\"\n",
    "        img = load_dicom_array(img_path)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        if self.use_windowing:\n",
    "            # Multi-window approach for 3-channel input\n",
    "            img = get_brain_windows(img)\n",
    "        else:\n",
    "            # Single channel approach\n",
    "            img = normalize_hounsfield(img)\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "            # Convert to 3-channel for models expecting RGB\n",
    "            img = np.stack([img, img, img], axis=-1)\n",
    "        \n",
    "        # Optional: skull stripping (can be slow)\n",
    "        # img = remove_skull_artifacts(img)\n",
    "        \n",
    "        # Resize with padding to maintain aspect ratio\n",
    "        img = resize_with_padding(img, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=img)\n",
    "            img = augmented['image']\n",
    "        \n",
    "        # Get label (adjust based on your label structure)\n",
    "        label = row['label'] if 'label' in row else 0\n",
    "        \n",
    "        return img, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ad212",
   "metadata": {},
   "source": [
    "## Model-Specific Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_specific_preprocessing(model_name: str, img_size: int = 384):\n",
    "    \"\"\"Get model-specific preprocessing parameters\"\"\"\n",
    "    \n",
    "    preprocessing_configs = {\n",
    "        'efficientnetv2': {\n",
    "            'img_size': 384,  # EfficientNetV2 works well with 384\n",
    "            'normalize': {\n",
    "                'mean': [0.485, 0.456, 0.406],\n",
    "                'std': [0.229, 0.224, 0.225]\n",
    "            }\n",
    "        },\n",
    "        'convnext': {\n",
    "            'img_size': 384,  # ConvNeXt standard size\n",
    "            'normalize': {\n",
    "                'mean': [0.485, 0.456, 0.406],\n",
    "                'std': [0.229, 0.224, 0.225]\n",
    "            }\n",
    "        },\n",
    "        'swin': {\n",
    "            'img_size': 384,  # Swin Transformer preferred size\n",
    "            'normalize': {\n",
    "                'mean': [0.485, 0.456, 0.406],\n",
    "                'std': [0.229, 0.224, 0.225]\n",
    "            }\n",
    "        },\n",
    "        'maxvit': {\n",
    "            'img_size': 384,\n",
    "            'normalize': {\n",
    "                'mean': [0.5, 0.5, 0.5],\n",
    "                'std': [0.5, 0.5, 0.5]\n",
    "            }\n",
    "        },\n",
    "        'coatnet': {\n",
    "            'img_size': 384,\n",
    "            'normalize': {\n",
    "                'mean': [0.485, 0.456, 0.406],\n",
    "                'std': [0.229, 0.224, 0.225]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config = preprocessing_configs.get(model_name.lower(), {\n",
    "        'img_size': img_size,\n",
    "        'normalize': {\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca2a1d",
   "metadata": {},
   "source": [
    "## Test Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a370c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tta(model, img_tensor, device):\n",
    "    \"\"\"Apply test-time augmentation for better predictions\"\"\"\n",
    "    tta_transforms = [\n",
    "        lambda x: x,  # Original\n",
    "        lambda x: torch.flip(x, dims=[3]),  # Horizontal flip\n",
    "        lambda x: torch.rot90(x, k=1, dims=[2, 3]),  # 90 degree rotation\n",
    "        lambda x: torch.rot90(x, k=2, dims=[2, 3]),  # 180 degree rotation\n",
    "        lambda x: torch.rot90(x, k=3, dims=[2, 3]),  # 270 degree rotation\n",
    "    ]\n",
    "    \n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for transform in tta_transforms:\n",
    "            augmented = transform(img_tensor.to(device))\n",
    "            pred = model(augmented)\n",
    "            \n",
    "            # Reverse transformation for predictions if needed\n",
    "            # (not needed for classification, but important for segmentation)\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    final_pred = torch.stack(predictions).mean(dim=0)\n",
    "    return final_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
